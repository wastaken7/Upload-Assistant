# -*- coding: utf-8 -*-
import aiofiles
import asyncio
import glob
import httpx
import os
import platform
import re
import unicodedata
from bs4 import BeautifulSoup
from src.bbcode import BBCODE
from src.console import console
from src.cookie_auth import CookieValidator, CookieAuthUploader
from src.get_desc import DescriptionBuilder
from src.languages import process_desc_language


class FF:
    def __init__(self, config):
        self.config = config
        self.cookie_validator = CookieValidator(config)
        self.cookie_auth_uploader = CookieAuthUploader(config)
        self.tracker = "FF"
        self.banned_groups = []
        self.source_flag = "FunFile"
        self.base_url = "https://www.funfile.org"
        self.torrent_url = f"{self.base_url}/details.php?id="
        self.requests_url = f"{self.base_url}/requests.php"
        self.auth_token = None
        self.session = httpx.AsyncClient(headers={
            'User-Agent': f"Upload Assistant/2.3 ({platform.system()} {platform.release()})"
        }, timeout=30.0)

    async def validate_credentials(self, meta):
        cookie_file = os.path.abspath(f"{meta['base_dir']}/data/cookies/{self.tracker}.txt")
        if not os.path.exists(cookie_file):
            await self.login(meta)

        self.session.cookies = await self.cookie_validator.load_session_cookies(meta, self.tracker)
        valid_cookies = await self.validate_cookies(meta)
        if valid_cookies:
            return True
        else:
            await self.login(meta)
            return await self.validate_cookies(meta)

    async def validate_cookies(self, meta):
        return await self.cookie_validator.cookie_validation(
            meta=meta,
            tracker=self.tracker,
            test_url=f'{self.base_url}/upload.php',
            success_text='friends.php',
        )

    async def login(self, meta):
        login_url = "https://www.funfile.org/takelogin.php"
        cookie_file = os.path.abspath(f"{meta['base_dir']}/data/cookies/{self.tracker}.txt")

        payload = {
            "returnto": "/index.php",
            "username": self.config['TRACKERS'][self.tracker]['username'],
            "password": self.config['TRACKERS'][self.tracker]['password'],
            "login": "Login"
        }

        print(f"{self.tracker}: Trying to login...")
        response = await self.session.post(login_url, data=payload)

        if response.status_code == 302:
            print(f"{self.tracker}: Login Successful!")

            async with aiofiles.open(cookie_file, "w") as f:
                await f.write("# Netscape HTTP Cookie File\n")
                await f.write("# This file was generated by an automated script.\n\n")
                for cookie in self.session.cookies.jar:
                    domain = cookie.domain
                    include_subdomains = "TRUE" if domain.startswith('.') else "FALSE"
                    path = cookie.path
                    secure = "TRUE" if cookie.secure else "FALSE"
                    expires = str(int(cookie.expires)) if cookie.expires else "0"
                    name = cookie.name
                    value = cookie.value
                    await f.write(f"{domain}\t{include_subdomains}\t{path}\t{secure}\t{expires}\t{name}\t{value}\n")
            print(f"{self.tracker}: Saving the cookie file...")
        else:
            print(f"{self.tracker}: Login failed. Status code: {response.status_code}")

    async def search_existing(self, meta, disctype):
        self.session.cookies = await self.cookie_validator.load_session_cookies(meta, self.tracker)

        if meta['category'] == 'MOVIE':
            query = meta['title']
        if meta['category'] == 'TV':
            query = f"{meta['title']} {meta.get('season', '')}{meta.get('episode', '')}"

        search_url = f"{self.base_url}/suggest.php?q={query}"
        response = await self.session.get(search_url)

        if response.status_code == 200 and 'login' not in str(response.url):
            items = [line.strip() for line in response.text.splitlines() if line.strip()]
            return items

        return []

    async def get_requests(self, meta):
        if self.config['TRACKERS'][self.tracker].get('check_requests', False) is False:
            return False

        else:
            try:
                self.session.cookies = await self.cookie_validator.load_session_cookies(meta, self.tracker)
                category = self.get_type_id(meta)

                query_1 = meta['title']
                query_2 = meta['title'].replace(' ', '.')

                search_url_1 = f"{self.requests_url}?filter=open&category={category}&search={query_1}"

                if query_1 != query_2:
                    search_url_2 = f"{self.base_url}/requests.php?filter=open&category={category}&search={query_2}"
                    responses = await asyncio.gather(
                        self.session.get(search_url_1),
                        self.session.get(search_url_2)
                    )
                    response_results_text = responses[0].text + responses[1].text
                    responses[0].raise_for_status()
                    responses[1].raise_for_status()
                else:
                    response = await self.session.get(search_url_1)
                    response.raise_for_status()
                    response_results_text = response.text

                soup = BeautifulSoup(response_results_text, "html.parser")
                request_rows = soup.select("td.mf_content table tr")

                results = []
                for row in request_rows:
                    name_element = row.select_one("td.row3 nobr a b")
                    if not name_element:
                        continue

                    name = name_element.text.strip()
                    link_element = name_element.find_parent("a")
                    link = link_element["href"] if link_element else None

                    all_tds = row.find_all("td", class_="row3")
                    reward = all_tds[2].text.strip() if len(all_tds) > 2 else None

                    results.append({
                        "Name": name,
                        "Link": link,
                        "Reward": reward
                    })

                if results:
                    message = f"\n{self.tracker}: [bold yellow]Your upload may fulfill the following request(s), check it out:[/bold yellow]\n\n"
                    for r in results:
                        message += f"[bold green]Name:[/bold green] {r['Name']}\n"
                        message += f"[bold green]Reward:[/bold green] {r['Reward']}\n"
                        message += f"[bold green]Link:[/bold green] {r['Link']}\n\n"
                    console.print(message)

                return results

            except Exception as e:
                print(f"An error occurred while fetching requests: {e}")
                return []

    async def generate_description(self, meta):
        builder = DescriptionBuilder(self.config)
        desc_parts = []

        # Custom Header
        desc_parts.append(await builder.get_custom_header(self.tracker))

        # Logo
        logo_resize_url = meta.get('tmdb_logo', '')
        if logo_resize_url:
            desc_parts.append(f"[center][img]https://image.tmdb.org/t/p/w300/{logo_resize_url}[/img][/center]")

        # TV
        title, episode_image, episode_overview = await builder.get_tv_info(meta, self.tracker)
        if episode_overview:
            desc_parts.append(f'[center]{title}[/center]')

            if episode_image:
                desc_parts.append(f'[center]<a href="{episode_image}" target="_blank"><img src="{episode_image}" width="220"></a>[/center]')

            desc_parts.append(f'[center]{episode_overview}[/center]')

        # File information
        mediainfo = await builder.get_mediainfo_section(meta, self.tracker)
        if mediainfo:
            desc_parts.append(f'[pre]{mediainfo}[/pre]')

        bdinfo = await builder.get_bdinfo_section(meta)
        if bdinfo:
            desc_parts.append(f'[pre]{bdinfo}[/pre]')

        # User description
        desc_parts.append(await builder.get_user_description(meta))

        # Screenshot Header
        desc_parts.append(await builder.screenshot_header(self.tracker))

        # Screenshots
        images = meta.get('image_list', [])
        if images:
            screenshots_block = "[center]"
            for image in images:
                img_url = image['img_url']
                web_url = image['web_url']
                screenshots_block += f'<a href="{web_url}" target="_blank"><img src="{img_url}" width="220"></a> '
            screenshots_block += "[/center]"

            desc_parts.append(screenshots_block)

        # Tonemapped Header
        desc_parts.append(await builder.get_tonemapped_header(meta, self.tracker))

        # Signature
        desc_parts.append(f"[url=https://github.com/Audionut/Upload-Assistant][center][size=1]{meta['ua_signature']}[/size][/center][/url]")

        description = '\n\n'.join(part for part in desc_parts if part.strip())

        bbcode = BBCODE()
        description = description.replace("[user]", "").replace("[/user]", "")
        description = description.replace("[align=left]", "").replace("[/align]", "")
        description = description.replace("[right]", "").replace("[/right]", "")
        description = description.replace("[align=right]", "").replace("[/align]", "")
        description = bbcode.remove_sub(description)
        description = bbcode.remove_sup(description)
        description = description.replace("[alert]", "").replace("[/alert]", "")
        description = description.replace("[note]", "").replace("[/note]", "")
        description = description.replace("[hr]", "").replace("[/hr]", "")
        description = description.replace("[h1]", "[u][b]").replace("[/h1]", "[/b][/u]")
        description = description.replace("[h2]", "[u][b]").replace("[/h2]", "[/b][/u]")
        description = description.replace("[h3]", "[u][b]").replace("[/h3]", "[/b][/u]")
        description = description.replace("[ul]", "").replace("[/ul]", "")
        description = description.replace("[ol]", "").replace("[/ol]", "")
        description = description.replace("[hide]", "").replace("[/hide]", "")
        description = description.replace("•", "-").replace("“", '"').replace("”", '"')
        description = bbcode.convert_comparison_to_centered(description, 1000)
        description = bbcode.remove_spoiler(description)

        # [url][img=000]...[/img][/url]
        description = re.sub(
            r"\[url=(?P<href>[^\]]+)\]\[img=(?P<width>\d+)\](?P<src>[^\[]+)\[/img\]\[/url\]",
            r'<a href="\g<href>" target="_blank"><img src="\g<src>" width="\g<width>"></a>',
            description,
            flags=re.IGNORECASE
        )

        # [url][img]...[/img][/url]
        description = re.sub(
            r"\[url=(?P<href>[^\]]+)\]\[img\](?P<src>[^\[]+)\[/img\]\[/url\]",
            r'<a href="\g<href>" target="_blank"><img src="\g<src>" width="220"></a>',
            description,
            flags=re.IGNORECASE
        )

        # [img=200]...[/img] (no [url])
        description = re.sub(
            r"\[img=(?P<width>\d+)\](?P<src>[^\[]+)\[/img\]",
            r'<img src="\g<src>" width="\g<width>">',
            description,
            flags=re.IGNORECASE
        )

        description = bbcode.remove_extra_lines(description)

        async with aiofiles.open(
            f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}]DESCRIPTION.txt", 'w', encoding='utf-8'
        ) as description_file:
            await description_file.write(description)

        return description

    def get_type_id(self, meta):
        if meta.get('anime'):
            return '44'
        category = meta['category']

        if category == 'MOVIE':
            return '19'

        elif category == 'TV':
            return '7'

    def file_information(self, meta):
        vc = meta.get('video_codec', '')
        if vc:
            self.video_codec = vc.strip().lower()

        ve = meta.get('video_encode', '')
        if ve:
            self.video_encode = ve.strip().lower()

        vs = meta.get('source', '')
        if vs:
            self.video_source = vs.strip().lower()

        vt = meta.get('type', '')
        if vt:
            self.video_type = vt.strip().lower()

    def movie_type(self, meta):
        # Possible values: "XviD", "DVDR", "x264", "x265", "MP4", "VCD"
        if self.video_source == 'dvd':
            return "DVDR"

        if self.video_codec == 'hevc':
            return "x265"
        else:
            return "x264"

    def tv_type(self, meta):
        # Possible values: "XviD", "HR-XviD", "x264-SD", "x264-HD", "x265-SD", "x265-HD", "Web-SD", "Web-HD", "DVDR", "MP4"
        if self.video_source == 'dvd':
            return "DVDR"

        if self.video_source == 'web':
            if meta.get('sd'):
                return "Web-SD"
            else:
                return "Web-HD"

        if self.video_codec == 'hevc':
            if meta.get('sd'):
                return "x265-SD"
            else:
                return "x265-HD"
        else:
            if meta.get('sd'):
                return "x264-SD"
            else:
                return "x264-HD"

    def anime_type(self, meta):
        # Possible values: "TVSeries", "TVSpecial", "Movie", "OVA", "ONA", "DVDSpecial"
        if meta.get('tvmaze_episode_data', {}).get('season_number') == 0:
            return "TVSpecial"

        if self.video_source == 'dvd':
            return "DVDSpecial"

        category = meta['category']

        if category == 'TV':
            return "TVSeries"

        if category == 'MOVIE':
            return "Movie"

    def movie_source(self, meta):
        # Possible values: "DVD", "DVDSCR", "Workprint", "TeleCine", "TeleSync", "CAM", "BluRay", "HD-DVD", "HDTV", "R5", "WebRIP"
        mapping = {
            "dvd": "DVD",
            "dvdscr": "DVDSCR",
            "workprint": "Workprint",
            "telecine": "TeleCine",
            "telesync": "TeleSync",
            "cam": "CAM",
            "bluray": "BluRay",
            "blu-ray": "BluRay",
            "hd-dvd": "HD-DVD",
            "hdtv": "HDTV",
            "r5": "R5",
            "web": "WebRIP",
            "webrip": "WebRIP"
        }

        src = (self.video_source or "").strip().lower()
        return mapping.get(src, None)

    def tv_source(self, meta):
        # Possible values: "HDTV", "DSR", "PDTV", "TV", "DVD", "DvdScr", "BluRay", "WebRIP"
        mapping = {
            "hdtv": "HDTV",
            "dsr": "DSR",
            "pdtv": "PDTV",
            "tv": "TV",
            "dvd": "DVD",
            "dvdscr": "DvdScr",
            "bluray": "BluRay",
            "blu-ray": "BluRay",
            "web": "WebRIP",
            "webrip": "WebRIP"
        }

        src = (self.video_source or "").strip().lower()
        return mapping.get(src, None)

    def anime_source(self, meta):
        # Possible values: "DVD", "BluRay", "Anime Series", "HDTV"
        mapping = {
            "hdtv": "HDTV",
            "tv": "HDTV",
            "dvd": "DVD",
            "bluray": "BluRay",
            "blu-ray": "BluRay",
            "web": "Anime Series",
            "webrip": "Anime Series"
        }

        src = (self.video_source or "").strip().lower()
        return mapping.get(src, None)

    def anime_v_dar(self, meta):
        # Possible values: "16_9", "4_3"
        if meta.get('is_disc') != "BDMV":
            tracks = meta.get('mediainfo', {}).get('media', {}).get('track', [])
            for track in tracks:
                if track.get('@type') == "Video":
                    dar_str = track.get('DisplayAspectRatio')
                    if dar_str:
                        try:
                            dar = float(dar_str)
                            return "16_9" if dar > 1.34 else "4_3"
                        except (ValueError, TypeError):
                            return "16_9"

            return "16_9"
        else:
            return "16_9"

    def anime_v_codec(self, meta):
        # Possible values: "x264", "h264", "XviD", "DivX", "WMV", "VC1"
        if self.video_codec == 'vc-1':
            return "VC1"

        if self.video_encode == 'h.264':
            return "h264"
        else:
            return 'x264'

    def edit_name(self, meta):
        is_scene = bool(meta.get('scene_name'))
        torrent_name = meta['scene_name'] if is_scene else meta['name']

        name = torrent_name.replace(':', '-')
        name = unicodedata.normalize("NFKD", name)
        name = name.encode("ascii", "ignore").decode("ascii")
        name = re.sub(r'[\\/*?"<>|]', '', name)

        return name

    async def languages(self, meta):
        if not meta.get('language_checked', False):
            await process_desc_language(meta, desc=None, tracker=self.tracker)

        lang_map = {
            'english': 'en',
            'japanese': 'jp',
            'korean': 'kr',
            'thai': 'th',
            'chinese': 'zh',
        }

        anime_a_codec = []
        anime_a_ch = []
        anime_a_lang = []

        anime_s_format = []
        anime_s_type = []
        anime_s_lang = []

        audio_languages = meta.get('audio_languages', [])
        if audio_languages:
            audio_desc = meta.get('audio', '').lower()
            found_codec = '0'
            codec_options = {
                'aac': 'aac', 'ac3': 'ac3', 'dd': 'ac3', 'dolby digital': 'ac3', 'ogg': 'ogg', 'mp3': 'mp3',
                'dts-es': 'dtses', 'dtses': 'dtses', 'dts': 'dts', 'flac': 'flac', 'pcm': 'pcm', 'wma': 'wma'
            }
            for key, value in codec_options.items():
                if key in audio_desc:
                    found_codec = value
                    break

            channels_desc = meta.get('channels', '')
            channel_map = {
                '2.0': '2',
                '5.1': '5_1',
                '7.1': '7_1'
            }
            found_channel = channel_map.get(channels_desc, '0')

            for lang_str in audio_languages:
                lang_code = lang_map.get(lang_str.lower(), '1')

                anime_a_codec.append(found_codec)
                anime_a_ch.append(found_channel)
                anime_a_lang.append(lang_code)

        subtitle_languages = meta.get('subtitle_languages', [])
        if subtitle_languages:
            subtitle_format = 'srt'
            subtitle_type = 'sub'

            for lang_str in subtitle_languages:
                lang_code = lang_map.get(lang_str.lower(), '1')

                anime_s_format.append(subtitle_format)
                anime_s_type.append(subtitle_type)
                anime_s_lang.append(lang_code)

        return {
            'anime_a_codec': anime_a_codec,
            'anime_a_ch': anime_a_ch,
            'anime_a_lang': anime_a_lang,
            'anime_s_format': anime_s_format,
            'anime_s_type': anime_s_type,
            'anime_s_lang': anime_s_lang,
        }

    async def get_poster(self, meta):
        poster_url = meta.get('poster')

        poster_file = None
        if poster_url:
            async with httpx.AsyncClient() as client:
                response = await client.get(poster_url)
                if response.status_code == 200:
                    poster_ext = os.path.splitext(poster_url)[1] or ".jpg"
                    poster_filename = f"{meta.get('name')}{poster_ext}"
                    poster_file = (poster_filename, response.content, "image/jpeg")

                    return poster_file

    def get_nfo(self, meta):
        nfo_dir = os.path.join(meta['base_dir'], "tmp", meta['uuid'])
        nfo_files = glob.glob(os.path.join(nfo_dir, "*.nfo"))

        if nfo_files:
            nfo_path = nfo_files[0]

            return {
                'nfo': (
                    os.path.basename(nfo_path),
                    open(nfo_path, "rb"),
                    "application/octet-stream"
                )
            }
        return {}

    async def get_data(self, meta):
        languages = await self.languages(meta)
        self.file_information(meta)

        data = {
            'MAX_FILE_SIZE': 10000000,
            'type': self.get_type_id(meta),
            'tags': '',
            'descr': await self.generate_description(meta),
        }

        if meta.get('anime'):
            data.update({
                'anime_type': self.anime_type(meta),
                'anime_source': self.anime_source(meta),
                'anime_container': 'mkv',
                'anime_v_res': meta.get('resolution'),
                'anime_v_dar': self.anime_v_dar(meta),
                'anime_v_codec': self.anime_v_codec(meta),
                'anime_a_codec[]': ['0'] + languages.get('anime_a_codec'),
                'anime_a_ch[]': ['0'] + languages.get('anime_a_ch'),
                'anime_a_lang[]': ['0'] + languages.get('anime_a_lang'),
                'anime_s_format[]': ['0'] + languages.get('anime_s_format'),
                'anime_s_type[]': ['0'] + languages.get('anime_s_type'),
                'anime_s_lang[]': ['0'] + languages.get('anime_s_lang'),
            })

        else:
            if meta['category'] == 'MOVIE':
                data.update({
                    'movie_type': self.movie_type(meta),
                    'movie_source': self.movie_source(meta),
                    'movie_imdb': str(meta.get('imdb_info', {}).get('imdb_url', '')),
                    'pack': 0,
                })

            if meta['category'] == 'TV':
                data.update({
                    'tv_type': self.tv_type(meta),
                    'tv_source': self.tv_source(meta),
                    'tv_imdb': str(meta.get('imdb_info', {}).get('imdb_url', '')),
                    'pack': 1 if meta.get('tv_pack', 0) else 0,
                })

        return data

    async def upload(self, meta, disctype):
        self.session.cookies = await self.cookie_validator.load_session_cookies(meta, self.tracker)
        data = await self.get_data(meta)
        torrent_name = self.edit_name(meta)
        files = {}
        files['poster'] = await self.get_poster(meta)
        nfo = self.get_nfo(meta)
        if nfo:
            files['nfo'] = nfo['nfo']

        await self.cookie_auth_uploader.handle_upload(
            meta=meta,
            tracker=self.tracker,
            source_flag=self.source_flag,
            torrent_url=self.torrent_url,
            data=data,
            torrent_field_name='file',
            torrent_name=torrent_name,
            upload_cookies=self.session.cookies,
            upload_url=f"{self.base_url}/takeupload.php",
            id_pattern=r'details\.php\?id=(\d+)',
            success_status_code=302,
            additional_files=files
        )

        return
